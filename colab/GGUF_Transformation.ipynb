{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 0. 환경 설정 (필요 패키지 설치)\n",
        "# ====================================================\n",
        "!pip install -U accelerate peft bitsandbytes\n",
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "5_PgoaufbEib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transformers repo clone 및 설치\n",
        "!git clone https://github.com/huggingface/transformers.git\n",
        "%cd transformers\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "ImYk8ux1kqgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login, HfApi, upload_file\n",
        "login()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "P4upvd8ChUDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 1. 사용자 설정\n",
        "# ====================================================\n",
        "model_list = [\n",
        "    {\n",
        "        \"model_id\": \"LeannaJ/gemma3n-lora-summary\",\n",
        "        \"repo_id_gguf\": \"LeannaJ/gemma3n-lora-gguf-summary\",\n",
        "        \"local_model_path\": \"/content/drive/MyDrive/Gemma_FineTuning/GGUF/gguf_input_summary\",\n",
        "        \"gguf_output_dir\": \"/content/drive/MyDrive/Gemma_FineTuning/GGUF/gguf_output_summary\"\n",
        "    },\n",
        "    {\n",
        "        \"model_id\": \"LeannaJ/gemma3n-lora-math\",\n",
        "        \"repo_id_gguf\": \"LeannaJ/gemma3n-lora-gguf-math\",\n",
        "        \"local_model_path\": \"/content/drive/MyDrive/Gemma_FineTuning/GGUF/gguf_input_math\",\n",
        "        \"gguf_output_dir\": \"/content/drive/MyDrive/Gemma_FineTuning/GGUF/gguf_output_math\"\n",
        "    }\n",
        "]\n",
        "\n",
        "quant_method = \"q4_0\"  # 또는 \"f16\", \"q8_0\""
      ],
      "metadata": {
        "id": "F4IWIgLTbH8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 2. GGUF 변환 실행 (transformers/scripts/convert/gguf/convert.py)\n",
        "# ====================================================\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import os, glob\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "for m in model_list:\n",
        "    print(f\"\\n🚀 변환 시작: {m['model_id']}\")\n",
        "\n",
        "    # 디렉토리 준비\n",
        "    os.makedirs(m[\"local_model_path\"], exist_ok=True)\n",
        "    os.makedirs(m[\"gguf_output_dir\"], exist_ok=True)\n",
        "\n",
        "    # 모델 다운로드 및 저장\n",
        "    tokenizer = AutoTokenizer.from_pretrained(m[\"model_id\"])\n",
        "    model = AutoModelForCausalLM.from_pretrained(m[\"model_id\"])\n",
        "    tokenizer.save_pretrained(m[\"local_model_path\"])\n",
        "    model.save_pretrained(m[\"local_model_path\"])\n",
        "\n",
        "    # GGUF 변환 실행\n",
        "    !python3 scripts/convert/gguf/convert.py \\\n",
        "      --model_path {m[\"local_model_path\"]} \\\n",
        "      --outfile_path {m[\"gguf_output_dir\"]} \\\n",
        "      --quantize {quant_method}\n",
        "\n",
        "    print(f\"✅ GGUF 변환 완료: {m['gguf_output_dir']}\")\n",
        "\n",
        "    # GGUF 파일 업로드\n",
        "    gguf_files = glob.glob(f\"{m['gguf_output_dir']}/*.gguf\")\n",
        "    api.create_repo(m[\"repo_id_gguf\"], exist_ok=True)\n",
        "\n",
        "    for filepath in gguf_files:\n",
        "        filename = os.path.basename(filepath)\n",
        "        upload_file(\n",
        "            path_or_fileobj=filepath,\n",
        "            path_in_repo=filename,\n",
        "            repo_id=m[\"repo_id_gguf\"],\n",
        "            repo_type=\"model\"\n",
        "        )\n",
        "        print(f\"✅ 업로드 완료: {filename} → {m['repo_id_gguf']}\")\n",
        "\n",
        "    print(f\"🌐 모델 업로드 완료: https://huggingface.co/{m['repo_id_gguf']}\")"
      ],
      "metadata": {
        "id": "6MY0pLWJDPRF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}