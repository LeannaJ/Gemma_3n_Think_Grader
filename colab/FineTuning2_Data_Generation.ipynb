{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOtkwSkyjVe-"
   },
   "source": [
    "## Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AAQwhOupiiTP"
   },
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# 0. 설치 및 기본 설정\n",
    "# ===================================================\n",
    "!pip install transformers datasets --quiet\n",
    "\n",
    "import re, torch, random\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import pipeline\n",
    "\n",
    "# ===================================================\n",
    "# 1. 데이터셋 로딩 및 샘플링\n",
    "# ===================================================\n",
    "print(\"🔄 Loading and sampling MathX-5M...\")\n",
    "streaming_dataset = load_dataset(\"XenArcAI/MathX-5M\", split=\"train\", streaming=True)\n",
    "subset = list(streaming_dataset.take(10000))  # Random 10,000\n",
    "\n",
    "# ===================================================\n",
    "# 2. LLM으로 학생 답변 생성 (OpenAI 또는 Gemma 가능)\n",
    "# ===================================================\n",
    "# GPT API 버전 예시 (OpenAI 사용 시)\n",
    "from openai import OpenAI\n",
    "\n",
    "openai = OpenAI(api_key=\"YOUR_OPENAI_API_KEY\")\n",
    "\n",
    "def generate_student_answer_gpt(problem):\n",
    "    prompt = f\"Solve the following problem and give only the final answer: {problem}\"\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return \"ERROR\"\n",
    "\n",
    "# 또는 Hugging Face Pipeline 사용 시 (Gemma)\n",
    "# from transformers import pipeline\n",
    "# llm = pipeline(\"text-generation\", model=\"google/gemma-2b-it\", device_map=\"auto\")\n",
    "\n",
    "# def generate_student_answer(problem):\n",
    "#     output = llm(f\"Solve: {problem}\", max_new_tokens=20)\n",
    "#     return output[0][\"generated_text\"].split(\"\\n\")[0].strip()\n",
    "\n",
    "# ===================================================\n",
    "# 3. 정답 비교 함수\n",
    "# ===================================================\n",
    "def clean_number(text):\n",
    "    # 수학적 숫자만 추출 (기본형)\n",
    "    matches = re.findall(r\"-?\\d+(?:\\.\\d+)?\", text)\n",
    "    return matches[0] if matches else None\n",
    "\n",
    "def is_correct(student_answer, expected_answer):\n",
    "    try:\n",
    "        sa = clean_number(student_answer)\n",
    "        ea = clean_number(expected_answer)\n",
    "        return int(float(sa)) == int(float(ea))\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# ===================================================\n",
    "# 4. 데이터셋 생성 (문제 + 답변 + 라벨)\n",
    "# ===================================================\n",
    "print(\"🧠 Generating answers and assigning labels...\")\n",
    "formatted = []\n",
    "\n",
    "for sample in subset:\n",
    "    problem = sample[\"problem\"]\n",
    "    expected = sample[\"expected_answer\"]\n",
    "    student = generate_student_answer_gpt(problem)\n",
    "\n",
    "    label = 1 if is_correct(student, expected) else 0\n",
    "    formatted.append({\n",
    "        \"input\": f\"Q: {problem}\\nStudent Answer: {student}\",\n",
    "        \"label\": label\n",
    "    })\n",
    "\n",
    "# ===================================================\n",
    "# 5. Class Balance 조정 (0/1 비율 1:1 맞추기)\n",
    "# ===================================================\n",
    "df = pd.DataFrame(formatted)\n",
    "df = df[df[\"label\"].isin([0,1])]\n",
    "\n",
    "correct = df[df[\"label\"] == 1]\n",
    "wrong = df[df[\"label\"] == 0]\n",
    "\n",
    "min_class_size = min(len(correct), len(wrong))\n",
    "balanced = pd.concat([correct.sample(min_class_size), wrong.sample(min_class_size)])\n",
    "\n",
    "balanced = balanced.sample(frac=1).reset_index(drop=True)\n",
    "print(f\"✅ Balanced dataset size: {len(balanced)} (1s: {sum(balanced.label==1)}, 0s: {sum(balanced.label==0)})\")\n",
    "\n",
    "# 저장 또는 학습용 Dataset 변환\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "classification_dataset = Dataset.from_pandas(balanced)\n",
    "classification_dataset.save_to_disk(\"/content/drive/MyDrive/Gemma_FineTuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bws4ah2tjdoz"
   },
   "source": [
    "## Reading/Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFJb57Hiim3w"
   },
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# 0. Install dependencies (Colab)\n",
    "# ===================================================\n",
    "!pip install transformers datasets sentence-transformers accelerate peft bitsandbytes trl --quiet\n",
    "\n",
    "import torch, re\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig,\n",
    "                          TrainingArguments, DataCollatorWithPadding)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# ===================================================\n",
    "# 1. Load CNN/DailyMail and Sample 2000\n",
    "# ===================================================\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train[:2000]\")\n",
    "\n",
    "# ===================================================\n",
    "# 2. Load Summarization Model (T5-small for speed)\n",
    "# ===================================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n",
    "# ===================================================\n",
    "# 3. Generate Student Summaries\n",
    "# ===================================================\n",
    "print(\"🔄 Generating student summaries...\")\n",
    "student_summaries = []\n",
    "\n",
    "for item in dataset:\n",
    "    input_text = \"summarize: \" + item[\"article\"][:1500]  # truncate long articles\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_new_tokens=64)\n",
    "    summary = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    student_summaries.append(summary)\n",
    "\n",
    "# ===================================================\n",
    "# 4. Embed & Score via Similarity (0~5 scale)\n",
    "# ===================================================\n",
    "print(\"🧠 Scoring with similarity...\")\n",
    "scorer = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "examples = []\n",
    "\n",
    "def get_score(student, reference):\n",
    "    sim = cosine_similarity([scorer.encode(student)], [scorer.encode(reference)])[0][0]\n",
    "    if sim > 0.85:\n",
    "        return 5\n",
    "    elif sim > 0.75:\n",
    "        return 4\n",
    "    elif sim > 0.65:\n",
    "        return 3\n",
    "    elif sim > 0.55:\n",
    "        return 2\n",
    "    elif sim > 0.45:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "for i, item in enumerate(dataset):\n",
    "    article = item[\"article\"]\n",
    "    reference = item[\"highlights\"]\n",
    "    student = student_summaries[i]\n",
    "    label = get_score(student, reference)\n",
    "    examples.append({\n",
    "        \"input\": f\"Article: {article}\\n\\nStudent Summary: {student}\",\n",
    "        \"label\": label\n",
    "    })\n",
    "\n",
    "# ===================================================\n",
    "# 5. Convert to HF Dataset and Save\n",
    "# ===================================================\n",
    "df = pd.DataFrame(examples)\n",
    "data = Dataset.from_pandas(df)\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "data.save_to_disk(\"/content/drive/MyDrive/Gemma_FineTuning/essay_eval_multiclass_2000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "syrdw_MTjHoG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNyaVCST06XI7k7HqyRNQwT",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
